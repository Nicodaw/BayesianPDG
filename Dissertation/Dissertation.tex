\documentclass{UoYCSproject}
\usepackage{outlines}
\usepackage{tikz}
\usepackage{booktabs,multirow}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{amsmath,amssymb}
\usepackage[ruled,vlined]{algorithm2e}
\addbibresource{bibl.bib}
\author{Angel Simeonov}
\title{Networks and Dragons: A data-driven approach to procedural dungeon generation}
\date{\today}
\supervisor{Dr. Rob Alexander}
\BEng

\dedication{<insert dedication>}

\acknowledgements{
  <insert acknowledgements>
}

% More definitions & declarations in example.ldf

\begin{document}
\pagenumbering{roman}
\maketitle
\listoffigures
\listoftables
%\renewcommand*{\lstlistlistingname}{List of Listings}
%\lstlistoflistings

\begin{summary}
>At most two (2) pages, aimed a non-specialist, knowledgeable authorial peer.
The summary must:
--state the aim of the reported work,
--motivate the work,
--state methods used,
--state results found, and
--highlight any legal, social, ethical, professional, and commercial issues as appropriate to the topic of study (if none, then this should be explicitly stated).
\end{summary}

\chapter{Introduction}
\label{cha:Introduction}

\section{Role-playing games}

\paragraph{}
Role-playing game (RPG) is a broad term encompassing a multitude of different games with often distinct mechanics and platforms of interaction (the media). The common factor between all RPGs is that the player(s) portray a fictional character and is involved in a fictional world (or a subset of one). The interaction of the players with this world is governed by rules, defined by the media. To better understand the structure of RPGs, we can view it in terms of two sets: 
\begin{outline}[enumerate]
  \1 Rules defining how we play the game. Describe the allowed actions for the player at any given moment in the game.
  \1 Narrative elements. Provide the \textit{purpose} of the players to interact with the fictional world. 
\end{outline}
The first set can be viewed as "How I interact with the environment" (functional) and the second as "What is the meaning of the environment" (narrative).

\paragraph{}
Computer RPGs like the Action RPG (ARPG) Legend of Zelda, Diablo, Fallout and many alike have both sets of rules defined by the game designers. A functional rule in an RPG like Skyrim is that you can attack with the left mouse button and a story element is that you are a Dragonborn with a quest. This quest is defined by the writers and designers of the game and as such exploring the narrative in a digital RPG can be comparable to an interactive reading of a fiction book. Good computer RPGs often have the ability to relax the narrative \parencite{TychsenGM}, allowing for the player to have more freedom in the exploration of the world and as such create the sensation that the player has some impact on this pre-programmed fictional environment.

\paragraph{}
The ability for a player to influence the narrative is one of the defining features of tabletop RPGs (TRPG a.k.a pen-and-paper PnP) like Dungeons and Dragons \parencite{DnD}. In them, the functional rules are usually defined by a rulebook (like the Player’s Handbook) and players verbally describe their interactions with the environment. The narrative is an ever evolving amalgam between the input of players and the Dungeon Master (DM). The DM’s task is to create a narrative outline and guide the player interaction. Because of the verbal nature of the game, the narrative does not suffer the limitations of its digital counterparts. But because there are no hard constraints to how the narrative is told, the DM has the non-trivial task of introducing consistency and outlining a structure for the story that would result in a compelling and ideally immersive experience for the players. This is often achieved by focusing the adventure’s act on a particular and detailed location. These locations are often referred to as Dungeons and in practice can be anything from the villain's mansion or a beast’s cave to a city under siege. A good dungeon design is crucial for creating a compelling narrative. The creative task of making the Dungeon is a laborious process. To facilitate that, academics and various gaming communities have been exploring different ways of automating Dungeon creation. We will refer to this process as Procedural Dungeon Generation (PDG). Arguably the greatest problem posed by automating PDG is answering the question of “What is a compelling Dungeon?”. In the next section we will review different generative methods for Dungeons and their associated limitations in an attempt to answer this question.

\section{State of the Art Generative methods}
% \begin{outline}
%   \1 Review various different algorithms for PDG and discuss their take on solving the "is this dungeon interesting?" problem.
%     \2 Non-digital: [[the Advanced DnD DM design kit book 3: Adventure Cookbook]]. Using dice tables to create different elements of the dungeon. Laborious process which requires the DM to remove any inconsistencies. Does not provide an actual Dungeon structure.
%     \2 Cellular automata: donjon \parencite{donjonPDG}. Issues associated with completely random topology and random content generation. Evaluated only on if the level is solvable (no evaluation of semantic content).
%     \2 Constraint Propagators and their related issues when formalising narrative as constraints.
%     \2 BPTs + semantic additions[[citation needed Thrall and Brown]]. Difficulty proving the 'goodness' of the generator. Discuss difficulties with an objective HCI evaluation for PnP (i.e. difficult to get a decent sample size, because playing a game is time consuming).
%     \2 Formal language approach. Grammars \parencite{DormansMS,Deery,Cadogan} as a natural way of describing narrative structures.
%     \2 Data-driven approaches. \parencite{SummervilleLearningOfZelda,SummervillePCGML,SummervilleSamplingHyrule} Relate the notion of learning from human-made dungeons as a way to create good structure + narrative. Note the lack of data. Expand more in the Mission and Space --> Aim sections
% \end{outline}
\subsection{Non-digital Generators}
\paragraph{}
The designers of the classic PnP modules acknowledge the issue of Dungeon creation and often incorporate \textit{Loot} or \textit{Encounter} tables in the rule-books. The tables are a reference over a set of treasures or monsters respectively, which can be chosen by rolling the specified die \textbf{need figure of loot/encounter table}. As noted before, a PnP RPG can evolve rapidly outside the planned narrative structure prepared by the DM and a simple way to quickly define new adaptive story elements via a loot table can be useful. Some early modules even provided a step-by-step guide for creating a full campaign from the plot, villains' obsessions and story setting to the various encounters and and treasures \parencite{ADnD} entirely based on randomised content tables. An issue arises when using loot tables when a randomly selected element from one table is contradicted by an item selected from another. \textbf{show an example from the ADND module}. It is up to the DM to resolve such disparities. Although this process provides some degree of creative assistance, it does little to facilitate the laborious nature of creating an end-to-end compelling narrative. Furthermore, as these methods are occupied with providing inspiration for global narrative elements, they pay no attention to providing guidance of what would mean a "good" Dungeon.

\subsection{Digital Generators}
\paragraph{}
It is important to note that PnP RPGs and the various genres of computer RPGs share the same narrative goal despite differing in mechanisms of interaction \parencite{Tychsen2006}. Therefore we will not limit ourselves to looking only at existing tabletop solutions.
Unlike the original non-digital generators, computerised ones rarely allow for human input during the generation process. A generative algorithm would provide a DM with an interface that takes a set of parameters and produce a template of a game. The degree of complexity of this template is naturally dependent on the complexity of the algorithm itself. Because they aim to exclude the human designer from the process, the digital generators tend to focus on creating elaborate Dungeon spaces and either provide basic semantic content or ommit it entirely.

\subsubsection{Cellular Automata}
Cellular automata (CA) utilises a procedural generation mechanism in which a $N \times M$ grid space is mutated incrementally by a set of agents\textbf{[citation needed for CA and fig. showing agents in action?]]}. The implementation of these mutation operators define if the CA will simulate erosion \parencite{donjonCA} or man-made structures such as rooms and corridors. The latter is the case of the donjon dungeon generator \parencite{donjonPDG}. Its simplicity and degrees of customisations of both layout and style has made donjon a popular generator choice with more than 2500 generated dungeons in the last 12 hours and 100 donations in the last 3 months \textbf{[[citation needed?]]}. Despite its popularity, donjon (as well as other CA PDGs) implores purely random generation techniques which are only evaluated based on the reachability (i.e. is there a path from the start to each room). Furthermore, although we have control over the input parameters for the topological randomness, the semantic elements (the contents, rather than the structure) of the dungeon are entirely arbitrary and often contradictory \parencite{Thrall,Brown}. In terms of the aforementioned reachability, donjon does not take in account the randomly allocated locked doors as the key allocation is left at the DM's discretion. That ultimately limits the usefulness of the tool as a narrative generator. %All information generated about the monsters, treasures and other quest elements are discarded by the DM \textbf{[[citation needed on how people use dungeon generators]]}

\subsubsection{Constraint Propagators}
Constraint Satisfaction Problems (CSPs) define a discrete set of variables with corresponding constraints that restrict the possible variable instantiations \textbf{[[cn  of CSPs]]}. Dungeon generation has been described as a CSP on occasions where the variables and constraints define the layout \textbf{[[cn]]}, contents \parencite{HorswillCSPMission} or both layout and content \parencite{GreenCSPboth} of a dungeon. Constraint Propagation (CP) is a particular method proven useful for solving CSP in the context of dungeon level generation. The algorithm selects a set of possible values for a particular variable and \textit{propagates} the result to the rest of the variables, adjusting their possible values accordingly. If the selected value narrows another variable's possible values to the empty set, we infer that this instantiation is suboptimal and we backpropagate to the last viable solution and retry. Furthermore it can be generalised that if an instantiation for all variables that does not narrow any variable to the empty set exists, the level is solvable. Utilising this formalisation of the \textit{solvability} of a level we can populate a level with varying degrees of complexity from ensuring that keys are always spawned before the door that they unlock to adjusting difficulty by measuring survivability metrics between rooms \textbf{[[cn]]}. Even thought CPs have solved some problems of the completely random generators, formulating a numerical constraint for a narrative element can be difficult. The complication is both in the translation of a story element to a numerical representation and in the computational expense that comes with computing large amounts of permutations.

\subsubsection{Binary Partitioning}
%TODO: bin partitioning, do we really need this? 
\subsubsection{Formal Languages}
%TODO: formal languages
\subsubsection{Data-driven approaches}
%TODO: Datadriven

\section{Mission and Space}
% \begin{outline}
%   \1 Introduce the notion of Mission and Space
%   \1 The reason for separating Mission and Space. We can model player experience better \parencite{DormansMS,SummervilleLearningOfZelda}.
% \end{outline}

\paragraph{}
An attentive reader would have noted that distinguishing between the level layout and contents is a common occurrence in popular PDG algorithms. This discrimination was formalised by Dormans \parencite{DormansMS} in his definitions of \textit{Mission} and \textit{Space} in relation to the ARPG Legend of Zelda game series.
The \textit{Space} of a dungeon represents its topological structure and can be encoded as an undirected cyclic graph where each vertex is a room and each edge is a door or corridor. The \textit{Mission} is the set of narrative tasks the player must accomplish in order for him to complete the dungeon. It can be encoded as a directed graph where each vertex is the objective and the edge directions show the required sequence of completion. How players interact with the level is governed entirely by the interaction of these two concepts. As noted by Dormans, a \textit{Mission} mapped on different layouts will result in drastically different exploration patterns and it is by understanding the necessities of the two separate entities that we can model player experience better. Due to this separation we have seen advancements in generating adaptable to the player game environments \parencite{DormansAE} and even reverse engineering the creation of \textit{Missions} and \textit{Spaces} by learning structure from data \parencite{SummervilleLearningOfZelda}.

\section{Aim}
%\begin{outline}
%  \1 The only \textit{"dungeon generator"} that has been empirically proven to have the ability of creating a truly compelling narrative is the human designer
%  \1 Discuss attempts at the data-driven approaches from \textbf{Generation Methods} in detail
%    \2 Deery's data inspired approach, but not data-driven
%    \2 Summerville's Learning of Zelda
%\end{outline}

\paragraph{}
The dungeon’s topology (Space) and contents (Mission) are correlated and embody the narrative of the game. As we have seen, dungeon generators usually implore bottom up approaches in which they apply rules for topology and then introduce dungeon content in an attempt to provide the structure for a captivating narrative. The various algorithmic methods have clear strengths and shortcomings in achieving the complex goal of creating an interesting story. From these observations it can be argued that that the ultimate \textit{dungeon generator} is the human designer. Dungeons and therefore narratives produced by humans are the most compelling out of all created dungeons. If we implore a top-down approach of analysing what makes a good man-made dungeon, we could potentially achieve higher levels of narrative automation.
\paragraph{}
Deery made the first steps by manually analysing submissions to the One Page Dungeon Contest \textit{OPDC} \parencite{OPDC} to extract a graph grammar for Mission generation \parencite{Deery}. The Mission graph was then mapped to a physical Space in a 1:1 ratio. The result was that each room was limited to a single Mission element. It is trivial to see that the originals in OPDC do not impose such a restriction. One room can have multiple Mission elements (e.g. the key to unlocking the door is on the bandit’s waist, Key + Encounter). Furthermore, Deery’s approach was to manually look at 10 competition winners and heuristically extract the grammar rules, which he highlights that they do not capture all the possible patterns. An automated approach to learning would potentially solve that issue. Programmatically learning a level from data has been an object of interest for computer based RPGs \parencite{SummervilleLearningOfZelda}, but has not been applied to PnP RPGs, presumably because of the lack of a consistent dataset.

\paragraph{}
In this paper we investigate if applying a data-driven approach to learning the Space of the dungeon can produce a map that is undistinguishable from human-made dungeon topologies. We discuss how we can use inference (Bayesian) networks to learn Space features from data. A review of the availability of data is conducted and complemented by assessing how meaningful features can be extracted. Subsequently we compare different network structures and learning algorithms and internally assess which model has the greatest statistical predictive capabilities using various scoring rules. We examine different methods that we can use for generating a dungeon topology from the best performant inference model. We conclude with a user study to externally validate how well the generator has approximated human-made dungeon topologies.

\chapter{Bayesian Inference Networks}
\label{ch:BNETS}
\paragraph{What are Bayesian networks?}
Bayesian Networks \textit{BNs} \parencite{pearl1985bayesian} (also referred to as Inference Networks, Belief Networks or Directed Acyclic Graph \textit{DAG} Models) are a graphical probabilistic model in which vertices are random variables \textit{RVs} whose edges indicate causal beliefs. BNs assert that our domain is defined in terms of a joint probability distribution \textit{PD} \(P(X_1, \ldots , X_n)\) over a finite set of RVs \(X\), each of which has a set of potential values it can take \(X_i(\Omega) = \{x_1, \ldots, x_j\}\) with an associated PD. The network structure shows how the full join distribution factorises given the causal dependencies. This property gives us a way to encode our prior domain knowledge about the relationships between our RVs.
Once we have encoded the RV interaction, using a BN as an inference tool is a matter of querying with a conditional set of RVs and noting how the conditional probability tables \textit{CPTs} change given our new knowledge. In practice, conditioning is done by \textit{observing} (i.e. instantiating) an RV to a particular value. Extracting information from our newly formed conditional distributions is done by sampling. Sampling involves simulating our network with respect to the observations in order to obtain point estimates. A detailed discussion of the sampling methodology will be provided in section \ref{sec:implementation}.

To contextualise the inference process for PnP RPGs, we can look into Summerville's work on the ARPG Legend of Zelda dungeons \parencite{SummervilleLearningOfZelda}. After annotating the dungeon maps with elements of both Mission and Space (total number of rooms, treasures, monsters, critical path CP, etc.), he extracts features for the BN by parsing them as RVs. Conditioning on total number of rooms by stating \(P(NumRooms = n) = 1\) results in a change of the PD for the possible CP values. An example logical outcome would be that the CPs that are greater than the total number of rooms would become impossible \(P(CriticalPathLength > n \mid NumRooms = n) \rightarrow 0 \). Then by sampling the network a point estimate for critical path length given that number of rooms in the dungeon is extracted. This \textit{observe-then-infer} process can be done with any permutation of our RVs.

\paragraph{Why use Bayesian networks?}
% causal structures gives us a natural interpretation of complex non-linear problems, compared to other methods (think NNs and their inability to be interpreted)
% they are fast once trained. Updating belief is a simple arithmetic operation on the different CPT
% gives us precise control over the parameters. Deery had an approximation of how big a dungeon should be because he would just repeat the rules X number of times. We can precisely fix every parameter that we have and even do permutations. This gives new degrees of freedom for the user to select the desired properties of his dungeon without compromising on automation.
The reasoning behind choosing BNs is that the probabilistic causal structures have been shown to perform well in capturing structural properties of dungeons from data for PDG tasks in ARPGs \parencite{SummervilleLearningOfZelda,SummervilleSamplingHyrule}. Furthermore they give us a natural description about the dependencies in our model, which can be used as a supplemental analysis tool for investigating human-made dungeons. As a PDG tool itself, the \textit{observe-then-infer} pattern gives the user precise control over the desired properties of his dungeon without loss of automation. If a user observes a dungeon with 5 rooms and critical path of length of 3, they will be guaranteed in the final product. This solves the problem previous graph grammar data-driven approaches encountered, whereby the dungeon size input was used as an approximation for the actual number of rooms in the dungeon \parencite{Deery}.

The downside of BNs is that their generalisation capabilities are highly dependent on the availability of data. If conditioning on an \textit{undefined} set (e.g. there was never a data entry that showed a 5 room dungeon with CP of 3), the inference will fail. Fortunately there are ways to work around this problem which we will discuss in section \ref{subsec:feature_extraction}.

A proposed way to solve this issue is by creating artificial data entries for the undefined cases. We will use linear interpolation due to its simplicity and the fact it has been shown to produce good results in general \parencite{Ibargengoytia2013OnTE} and exceptional results on small datasets \parencite{yu2004advances}. This can potentially solve the problems of undefined dungeons within the range of sizes of our dataset, but it cannot extrapolate to infer arbitrarily large dungeons. We would like to argue that this generalisation restriction is not a limitation of our algorithm, but of the data. Different dungeons exhibit different properties and it is not feasible to consider learning all possible existing dungeon configurations from a single dataset. This limitation is reflected in the dataset selection criteria in the next section.

\chapter{Methods and Implementation}
%TODO: Define notation
Features or RVs are denoted by capital letters \(X\) where a boldface \(\boldsymbol{X}\) is a shorthand for a set of RVs \(\{X_1, \ldots, X_n\}\)
Parameters refer to the network parameters, i.e. the conditional probaiblity tables CPTs defined by \(P(Y_1, \ldots, Y_n \mid X_1, \ldots , X_n)\)
Joint probability distributions \(P(X_1, \ldots , X_n)\)
Calligraphic \(\mathcal{N}\) denotes the CPTs for a given network
Calligraphic \(\mathcal{D}\) is a wrapper for all our data

\section{Data acquisition}
\subsection{Data selection} % what dataset should we choose:: OPDC winners for the last five years
% What problems can we encounter? Sparsity of data. Very different dungeons, some are unusable (say unusable criteria)
\paragraph{}
Unsurprisingly the first priority of a data-driven method is the acquisition of a suitable dataset. The criteria we have chosen for our data is the following:
\begin{outline}[enumerate]
  \1 Data must be consistent. Dungeons have to be representatives of a similar and comparable category.
  \1 Data must be available under a research or similar open-source license.
\end{outline}
The \textit{OPDC} supplies a dataset that satisfies our requirements. All dungeons are in the same category of a one-session long dungeon and each dungeon is issued under the Creative Commons CC license. Furthermore due to the fact that it is a competition, we have an exemplar answer of our main question "What makes a compelling dungeon?" in the form of the competition winners. We apply the first filter and review only entries by competition winners. Due to the creative freedom the human designers have, it is na\"{i}ve to assume that the dungeon Space is necessarily a dominant factor in all dungeons. In order to narrow down to dungeons that do have a consistent representation of Space as an important asset, we apply a second filter by omitting dungeons that have:
\begin{outline}[enumerate]
  \1 no map (no Space)
  \1 exclusively linear topologies
  \1 non-standard Space traversal (the map topology is ignored due to a functional mechanism e.g. time-travel, game of political control, king of the hill, etc.)
  \1 inconsistent map (our Space parameters change as the game progresses e.g. intrigue campaigns where the objective constantly changes locations)
  \1 no goal defined by the designer (introduces the same problem as 4.)
\end{outline}

\subsection{Extracting Features}
\label{subsec:feature_extraction}
\paragraph{}
We will base our feature selection on the work done by Summerville \parencite{SummervilleSamplingHyrule} by selecting Space related parameters from his Extreme Sparse model due to its simplicity and effectiveness. We extract five parameters in two categories: 
\begin{outline}[enumerate]
  \1 Dungeon (Global): total number of rooms \(R\) and how many rooms are on the critical path \(L\)
  \1 Room (Local): depth \(D\) (how far away the room is from the entrance), critical path distance \(S\) (how far away is the room from a critical path room, 0 indicates it is on the CP) , total number of neighbours \(N\)
\end{outline}
Each global and local set of parameters is defined by the joint PDs respectively \(P(R, L)\) and \(P(S, D, N)\). A full dungeon is therefore characterised by 

\begin{equation}
  \label{eq:full_joint_PD}
  P(R,L,S,D,N) = P(R, L) \prod_{r}^{\mid R \mid} P(S_r, D_r, N_r)
\end{equation}
where \(|R|\) is the total number of rooms defined by the instantiation of \(R\). To extract the five parameters, we need to transform the artistic and graphically rich textual representations of the dungeons in our dataset. Unfortunately automating feature extraction is very difficult due to the variant stylistic formatting and the prose like description of the tasks. Because of this, we establish a systematic way of manually annotating the Space of the dungeon as an abstract undirected graph from which we can calculate all the different parameters as shown in \textbf{[[insert figure of 2015 Sepulchre of the Abyss transcriptions]]}.

\paragraph{}
The dungeon sizes in our dataset range from two to 27, but due to our restrictions combined with chance, dungeons of some sizes were never recorded. Namely dungeons with sizes \(\{4,23\}\). We could conclude that lack of dungeons from those sizes means they are highly improbable to the point where we should not be concerned with their existence. This is what we do with dungeons that are outside of the range of sizes for one-page adventures. But in terms of utility of our tool we would like to be able generate any dungeon within that range. This issue of undefined data is the main drawback of inference network as outlined in chapter \ref{ch:BNETS}. Our proposed solution is to linearly interpolate \textit{LI} dungeons that are close to the missing ones in order for us to get estimates for the global parameters. We have chosen LI due to its simplicity and the fact it has been shown to produce good results in general \parencite{Ibargengoytia2013OnTE} and exceptional results on small datasets \parencite{yu2004advances}. Room (local) parameters could potentially also be linearly interpolated by using all the rooms from the neighbouring dungeons but this poses several questions about what does it mean to interpolate 5 rooms and 6 rooms. A better and more intuitive way to handle them is by leveraging the properties of the BNets. We can introduce the room parameters for the interpolated dungeons as unknown values to be inferred from the known data. We are treating the data itself as a parameter to be estimated from our other observations. This is a convenient way that inference networks treat missing values or sometimes even missing variables \textbf{[[ce on latent rvs]]}. However this representation of the unknown will impose a restriction on what CPT learning algorithms we can use. We discuss them in section \ref{subsec:param_learning}.

\paragraph{}
The final dataset is saved in a CSV format \texttt{DR_data.csv} and consists of 92 dungeon + 1193 room entries and 10 linearly interpolated dungeon entries with 189 unknown rooms. Common statistics of the dataset can be found in appendix \ref{cha:aA}.

\section{Model selection} % we have to choose a model structure denoting assumptions about dependencies then compute the CPTs
\label{sec:model_selection}

\paragraph{What we need to do to use Bayesian Networks?}
We need to define two key attributes of the BN:
\begin{outline}[enumerate]
  \1 The causal network's topology
  \1 Conditional probability tables \textit{CPTs} for each RV
\end{outline}
In this paper we will empirically compare different models in search of the one with highest statistical predictive capabilities. We will evaluate three structures with two CPT learning algorithms against a unform distribution (control) for a total of 7 models. The structures are the previously proposed Extreme Sparse model \parencite{SummervilleSamplingHyrule} (modified for Space only), an automatically learned structure using the Tree Augmented Na\"{i}ve Bayes and a fully connected model. The CPT learning algorithms will be Expectation Maximisation \textit{EM} and Gradient Descent \textit{GD}. Implementation of all BN related algorithms is done in the Netica Bayesian Network development software \parencite{netica}.

\subsection{Bayesian Network topology}
\subsubsection{Summerville's Extreme Sparse Model \textit{SESM}}
In exploring data-driven PDG for the Legend of Zelda franchise, Summerville found out that when dealing with a small dataset (38 dungeons with 1031 rooms), the SESM is able to capture high level properties of both Mission and Space with the highest accuracy in respect to the learning criteria and the lowest complexity. Following our assertion that digital RPGs differ from PnP RPGs only in mechanisms of interaction, we will base one of our BN topologies on Summerville's work. In order to do so, we flatten down the graph to use only the Space parameters. The flattening is done by reconstructing the SESM in Netica and applying the \texttt{AbsorbNodes\_bn} \parencite[62-63]{neticaCman}. The function removes the unused Mission RVs by effectively treating them as constants and propagating any implied dependencies from the other nodes. In this way we remove the unused RVs and maintain the joint distribution dependencies between our Space parameters. The transformation can be seen in figure \ref{fig:SESM}. The resulting model describes the following factorisation over our RVs
\begin{equation}
  \label{eq:SESM}
  P(R,L,S,D,N) = P(R)P(S)P(L \mid R)P(D \mid R, L)P(N \mid D, S)
\end{equation}

\begin{figure}[htb]
  \centering
  \begin{subfigure}[b]{0.55\textwidth}
    \centering
    \includegraphics[width=\textwidth]{SESM_full.png}
    \caption{Sumerville's Extreme Sparse Model}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.35\textwidth}
    \centering
    \includegraphics[width=\textwidth]{SESM_space.png}
    \caption{Space only Model}
  \end{subfigure}
  \caption{Expert defined Network topologies}
  \label{fig:SESM}
\end{figure}

\subsubsection{Tree Augmented Na\"{i}ve Bayes}
%TODO: make TAN clearer
% In the above approach we used specialist knowledge (Sumerville's experiments) to create the BN structure
% We made two crucial assumptions in that PnP == ARPG && flattening Mission to Space only will still retain the same operational meaning of the dungeon due to it retaining the same conditional dependency assumptions through the BN properties
% But we want to put those assumptions to the test by also learning the graph structure from the data we have.
\paragraph{}
In the above approach we applied specialist knowledge from Sumerville's experiments to create the BN structure. For that network to be valid, we asserted two assumptions: equivalence between PnP and ARPG dungeons and that flattening Mission to Space retains the same operational meaning due to the BN retaining the same conditional dependencies. To contest these somewhat ambitious assumptions, we will also create a BN by ignoring any domain knowledge we have and infer the structure purely based on the data using Tree Augmented Na\"{i}ve Bayes \textit{TAN} \parencite{FriedmanTAN}.  
\paragraph{}
Informally the learning procedure can be defined as finding the most likely BN structure that agrees with our data. \textbf{[[more formal description of maximising the posterior in respect to P(Network|Data)?]]}. The TAN algorithm starts with the usual na\"{i}ve Bayes model structure which states that all RVs are conditionally independent given one specific RV (the class RV). We will choose the class to be \(R\) due to its contextual importance. The na\"{i}ve assumption implies that
\begin{equation}
  \label{eq:TAN}
  P(R,\boldsymbol X) = P(R)\prod_{x}^{\boldsymbol X} P(x \mid R)
\end{equation}

where \(\boldsymbol X = \{L, S, D, N\}\). With this very restrictive assertion we, in practice, have a BN structure that we can use. But although the induced bias towards the class is often shown to be practically useful, in our case assuming that the room parameters are independent from each other does not make much sense. For precisely this reason, the TAN algorithm analyses our data and creates \textit{augmenting edges}. They are causal links connecting RVs that are found to have correlations. The final structure which can be seen in \ref{fig:TAN} is the original na\"{i}ve Bayes with the extra added augmenting edges, resulting in a model that has an informed causal structure and is alleviated from the restrictions of the na\"{i}ve assumption. It should be noted that the we did not discuss the \textit{tree} nature of TAN, because it is related to practical optimisations for learning large graphs with which we are not concerned.

\begin{figure}[htb]
  \centering
  \begin{subfigure}[b]{0.55\textwidth}
    \centering
    \includegraphics[width=\textwidth]{NBNet.png}
    \caption{Na\"{i}ve Bayes}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.35\textwidth}
    \centering
    \includegraphics[width=\textwidth]{TANNet.png}
    \caption{TAN}
  \end{subfigure}
  \caption{Learned Network topologies}
  \label{fig:TAN}
\end{figure}


\subsection{Fully Connected}
Due to the fact that we have a small number of nodes, we can afford the computational expense of running a model with no independence assumptions. In effect we're solving the original model \ref{eq:full_joint_PD}, but we will include a heuristic assumption about the direction of the causality in order to maintain the BN requirement of having an acyclic graph. Namely we assert that our global parameters cause our local parameters. The model can be seen in figure \ref{fig:FullyConnected}.

\begin{figure}[htb]
  \centering
    \includegraphics[width=\textwidth]{FullyConnected.png}
    \caption{Fully connected graph with heuristic causal flow}
  \label{fig:FullyConnected}
\end{figure}


\subsection{Parameter Learning}
\label{subsec:param_learning}
When we learn parameters (CPTs for each RV) we are trying to find the most likely PD for each RV that agrees with our data. More formally if \(\mathcal{N}\) is the network's CPTs and \(\mathcal{D}\) is the data then from Bayes's rule we know that \(P(\mathcal{N} \mid \mathcal{D}) = \frac{P(\mathcal{D} \mid \mathcal{N})P(\mathcal{N})}{P(\mathcal{D})}\). We can formulate the parameter learning as an optimisation problem where we find the best values for the posterior \(P(\mathcal{N} \mid \mathcal{D})\) by maximising the likelihood \(P(\mathcal{D} \mid \mathcal{N})\) given our prior \(P(\mathcal{N})\) \parencite[46-48]{neticaCman}. This is in fact the common maximum likelihood estimation MLE \textbf{[[cn for MLE]]}, but because we have missing data, \(P(\mathcal{D})\) is undefined and therefore the MLE is also undefined. To work around this issue and learn the likelihood we review two algorithms Netica has in its toolkit.
 
% \subsubsection{Count \parencite[48-50]{neticaCman}}
% The counting learning states that the probabilities in a single node are defined by how often a particular value has been seen in the data. This notion is defined as the \textit{experience} of that value. The network starts in a state of complete ignorance with uniform probabilities across all RVs. As we iteratively load in the data, the CPT are updated by increasing the probability for a particular value every time it is seen. Specifically, the experience is updated by the previous experience and the degree factor of that data point (i.e how often is it repeated, usually 1) \(e^{\prime} = e + d\). The CPT values are computed as a ratio of the previous and newly observed experience \(P^{\prime}(X=x) = \frac{P(X=x) \times e}{e^{\prime}}\). The counting algorithm is recommended due to its speed and simplicity, but it has a substantial drawback in the fact that it needs the data to be fully defined. Theoretically we should be able to use the counting algorithm as we have no latent (undefined) RVs nor missing data.

\subsubsection{Expectation Maximisation EM} %both EM and GD can get stuck on local minima
EM \parencite{EMAlgorithm} is a nummerical algorithm that can handle missing data. It assingns random probabilities to our missing values and iteratively alternates between maximising the parameters of the model given the data (Maximisation step) and finding better estimations of the missing data (Expectation step). It converges after a sertain set of predefined iterations or when no new updates are happening. 

\subsubsection{Gradient Descent GD}
%todo: finish GD
GD Netica specifically uses conjugate gradient descent \textbf{[[cn]]} whereby it maximises \(\)
Both EM and GD risk getting stuck in local optima.

\section{Implementation}
\label{sec:implementation}
\paragraph{}
The platform of choice for this project is .NET with the language of implementation being C\#. We chose this as Netica has native support for C\# via its COM API and because of the availability of useful third-party utilities. Namely we will be using Nepo\v{z}itek's PDG tool \parencite{levelGenerator} which allows us to input an abstract graph for the desired topology and produces a rendered image that is ready for player usage. The reason behind using a third-party solution for the dungeon rendering is due to time constraints and the fact that the visualiser is not a centre piece of this experiment. Our sole requirements are that the rendering is consistent and robust enough to handle our produced topologies. Nepo\v{z}itek's PDG satisfies these requirements \parencite{Nepozitek2018FASTCT}.
 %from chosen model --> dungeon graph, assumptions about constraints specifics etc
\paragraph{Dungeon Sampling} %What 
We can define characteristics of the desired dungeon by leveraging the inference nature of BNs. Generating a dungeon with five rooms is a matter of fixing (\textit{observing}) the \texttt{NumRooms} parameter. The user can choose virtually any permutation of parameters to be fixed or inferred. For consistency of this experiment, we are observing only the size of the dungeon via \texttt{NumRooms}.

\paragraph{Room Sampling}
The sequence of operations can be seen in appendix A \ref{algo:sampling}.
% fix dungeon, sample the rest
% Netica applies the Naive Bayes assumption that given the class variable NumRooms, all rooms are independent. The reason for this assumption is that we avoid the computational complexity of maintaining series of CPTs for each room instantiation. Why is this assumption not bad for us? Potentially because it's empirically shown to not decrease accuracy? https://www.norsys.com/WebHelp/NETICA/X_Bayesian_Learning.htm "Assuming the conditional probabilities to be independent generally results in poor performance when the number of usable cases isn’t large compared to the number of parent configurations of each node to be learned." We're safe because the ratio from parent (dungeon params) to children (room params) is count(dungeon.csv)/count(room.csv)

\paragraph{Converting from samples to Space}
%considerations.
%Propositions:
%1. Ad-hoc implementation : not pretty, cannot guarantee its valid without assuming soft constraints
%2. Heuristic search ==> I could not conceptualise an intuitive way to use search. Search over the space of potential room connections. The heuristic is going to be a production of the sampled parameters (sounds like CSP with an extra step?)
%3. Graph grammars  
% + intuitive, just apply graph production rules until we satisfy the sampled params.
% - Not extensible, if we create a representation now for Space, if we want to extend it to include Mission parameters we have to make the graph typed, which is not a trivial thing to do.
% - Our production rules (grammar) will be heuristically created.
%4. Constraint solvers
% + intuitive, the sampled params are our constraints.
% + Extensible: if expanding to Mission, we just add new constraints to be satisfied.
% + gives us an intuition about solvability, which is a bonus internal metric for our BNet. I.e. did we sample something that makes sense? 
% + can be generalised and optimised :: potentially translate it for general linear solvers
% todo: explain what the constraints are more than why we want this

\chapter{Results}
\label{cha:results}
%TODO: More info about the score rule? Explain the table, each column is one probability \(P(\theta \mid R)\)
\section{Internal Validation}
As we are modelling uncertainty, we can pose the question of "How well do we reduce uncertainty". This question can be answered by using the BNet as a classifier in which we only know the user input parameters (i.e. the size of the dungeon) and see how well we predict the rest of the parameters. We construct a within sample test where we measure how well the different models estimate \(P(L,S,D,N\mid R)\). We will use two metrics: the classification error \(E\) due to its natural meaning in conjunction with the well known quadratic loss (Brier score) \parencite{PearlScoringRules}. It should be noted that Netica uses the original formulation of the quadratic loss which is between zero and two. The comparison between our different models can be seen in table \ref{table:BNetCompare}. We observe that the TAN and fully connected networks both have the highest predictive capabilities therefore we will choose TAN to continue forward as it is the simpler structure. Our constraint propagation algorithm furhter reports that on average (sample size of 15) it takes 451 retries and 25 minutes to sample a single 10 room dungeon that satisfies our physical consistency requirements described in \ref{sec:implementation}.

\begin{table}[htb]
  \centering
  \resizebox{\textwidth}{!}{%
    \begin{tabular}{@{}llllllllll@{}}
      & \multicolumn{2}{l}{CPLength}       & \multicolumn{2}{l}{CPDistance}     & \multicolumn{2}{l}{Depth}          & \multicolumn{2}{l}{Neighbours}    &                      \\ \midrule
      & loss            & E                & loss            & E                & loss            & E                & loss            & E               &                      \\ \midrule
      Uniform (base)           & 0.9             & 96.74\%          & 0.8333          & 58.17\%          & 0.9286          & 92.04\%          & 0.8889          & 70.08\%         &                      \\ \midrule
      Extreme Sparse           & 0.5524          & 48.63\%          & 0.6744          & 58.17\%          & 0.8184          & 74.43\%          & 0.7486          & 67.64\%         &                      \\
      \textbf{TAN}             & \textbf{0.5524} & \textbf{48.63\%} & \textbf{0.6462} & \textbf{54.82\%} & \textbf{0.8184} & \textbf{74.43\%} & \textbf{0.7124} & \textbf{60.6\%} & EM                   \\
      \textbf{Fully Connected} & \textbf{0.5524} & \textbf{48.63\%} & \textbf{0.6464} & \textbf{54.82\%} & \textbf{0.8184} & \textbf{74.43\%} & \textbf{0.7131} & \textbf{60.6\%} &                      \\ \midrule
      Extreme Sparse           & 0.8132          & 52.1\%           & 0.7004          & 64.63\%          & 0.8689          & 78.46\%          & 0.7563          & 67.48\%         & \multicolumn{1}{c}{} \\
      TAN                      & 0.816           & 53.11\%          & 0.6751          & 57.84\%          & 0.8815          & 78.46\%          & 0.7686          & 65.88\%         & GD                   \\
      Fully Connected          & 0.8177          & 50.58\%          & 0.6855          & 57.84\%          & 0.877           & 79.3\%           & 0.7597          & 66.39\%         &                      \\ \bottomrule
    \end{tabular}
  }
  \caption{Structure and parameter learning comparison.}
  \label{table:BNetCompare}
  \end{table}

\section{External Validation} %user study
\paragraph{User study design}
Previous work the aesthetics of composition has shown that people can discriminate between computer generated and original artwork \parencite{McManusMondrian} \parencite{FurnhamMondrian}. To validate if our model is able to generate dungeons that are undistinguishable from human-made dungeons we will conduct a preference study. Dungeons from the OPDC, our inferred set and a random control group are selected and formatted in the same visual style. Three sets of pairs are randomly permuted and contain an even distribution of the possible combinations \textit{(OPDC,ours),(OPDC,random),(ours,random)}.

10, 13 and 8 room dungeons were used as they are the most common and additionally 13 is the mean dungeon size.
The experiment was done as an online survey.

\paragraph{Demographics}
The participants were selected to be adults with at least some experience in PnP RPGs. 
%TODO: Who did the study? Finish when the do the study


\chapter{Discussion}
%TODO: finish, talk about satisfiability
As expected, all models surpass the uniform distribution control. However the above 50\% error rate reveals that with the assumptions for making the problem description more concrete (section \ref{subsec:feature_selection}), we have removed crucial explanatory variables. The low predictive capabilities for the Space model is an indicator that the Mission parameters or some other unmodelled hidden set of parameters (perhaps a variable for creativity) are crucial when trying to approximate human-made dungeons. 


The EM algorithm consistently outperforms GD, albeit not by a large margin. 

\section{Critique}
\begin{outline}[enumerate]
  \1 Data
    \2 availability and extraction. 
      \3 Could extend to include even non-winning entries for the sake of having a greater sample size. More datasets can be considered. We've use OPDC due to its CC license, but paid modules exist.
      \3 Data extraction has been a manual process. Although due diligence is paid, noise introduced from human error is inevitable. 
  \1 Method
    \2 Cannot generalise to unseen cases. We can interpolate for missing data, but we cannot extrapolate. Argue that this is an issue of all ML approaches, not just BNets, because we're trying to capture properties of One-Page dungeon.
    \2 This paper is considering only Space gen. A more robust approach would be needed to extract consistent and meaningful parameters for Mission. For Legend of Zelda there is a discrete subset of things you can do so that is why Mission extraction is possible. Deery has shown that formalising Mission in PnP RPG's is difficult due to the variant and creative nature of the human-made dungeons.
  \1 Validation
    \2 We have decided to approach the external evaluation in a quantitative, rather qualitative measuring. That is due to the fact that conducting a study with a high environmental index is difficult due to multiple confound factors that are due to the nature of a tabletop RPG session. Not only does a one-session adventure usually take around 3 hours \textbf{[[cn]]}, but they are extremely variant between groups of players and DMs. An experiment that analyses how players use the generated dungeon rather than just discriminating between different dungeons topologies would give us more insight in the success of the recreation of a human-made dungeon.
  
\end{outline}

\chapter{Conclusion}
\label{cha:conclusion}

\subsection{Future Work}
\begin{outline}[enumerate]
  \1 Rerun with more data (OPDC is conducted every year)
  \1 Automate the Feature Extraction process (OCR + NLP)
  \1 Incorporate Mission parameters as well
  \1 Analyse PnP specific dungeons and find more parameters that might be useful. E.g. size of rooms, multiple CPs, 
  \1 Conduct a study with greater environmental impact (e.g. actually get people to play instead of using an aesthetics study)
\end{outline}


\appendix
\chapter{appendix}
\label{cha:aA}

\begin{figure}[htb]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/data_plots/dungeon_bp.eps}
    \caption{Global (dungeon)}
  \end{subfigure}
  ~
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/data_plots/room_bp.eps}
    \caption{Local (Dungeon)}
  \end{subfigure}
  \caption{Parameter statistics}
  \label{fig:dataStats}
\end{figure}

%TODO: write the algo
\begin{algorithm}
  \SetAlgoLined
  \DontPrintSemicolon
  \SetKwFunction{Observe}{Observe}\SetKwFunction{Sample}{Sample}\SetKwFunction{ClearObservations}{ClearObservations}
  \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
  \Input{BNetModel, int \(observed\_R\)}
  \Output{int estimates for the dungeon and each room parameter \((R, L, \{S_r, D_r, N_r\})\)}
  \BlankLine
  \(R \leftarrow \) BNetModel.\Observe{\(observed\_R\)}\;
  \(L \leftarrow \) BNetModel.\Sample{}\;
  \For{\(r \in R\) \KwDo}{
    \(temp\_R = R\)\;
    \(temp\_L = L\)\;
    \((S_r, D_r, N_r) \leftarrow \) BNetModel.\Sample{}\;
    BNetModel.\ClearObservations{}\;
    \(R \leftarrow \) BNetModel.\Observe{\(temp\_R\)}\;
    \(L \leftarrow \) BNetModel.\Observe{\(temp\_L\)}\;
   }
   \caption{Dungeon and Room sampling algorithm}
   \label{algo:sampling}
  \end{algorithm}

  \begin{algorithm}[H]
    \SetAlgoLined
    \DontPrintSemicolon
    \SetKwFunction{FMap}{Map}\SetKwFunction{FMapOne}{MapOne}\SetKwFunction{FPropagate}{Propagate}
    \SetKwFunction{FReduce}{Reduce}\SetKwFunction{FIsSingletonSet}{IsSingletonSet}\SetKwFunction{FUndo}{Undo}
    \SetKwFunction{FChooseRand}{ChooseRand}
    \SetKwProg{Fn}{Function}{:}{}
    \SetKwProg{try}{try}{}{}
    \SetKwProg{catch}{catch}{}{}
    \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
    \Input{set of rooms to be connected \(\mathcal{R}\), set of constraints for the dungeon and rooms \(\mathcal{C} = (R, L, \{S_r, D_r, N_r\})\)}
    \Output{A connected dungeon graph}
    
    \BlankLine
    \FMap{}\;

    \BlankLine
     \Fn{\FMap{}}{
      \For{\(r \in \mathcal{R}\)}{
        \(r_v \leftarrow\) all possible values for r\;
      }
      \FMapOne{}\;
      % \KwRet\;
     }

     \BlankLine
     %\tcc{Select a room \(r\) which still has more than one possible values in its set \(r_v\) and pick value for it}
     \Fn{\FMapOne{}}{
      \If{\(\forall r \mid\) \FIsSingletonSet{r_v}}{ %\If{} for normal ifs
        \KwRet\;
      }{
        \(r^\prime \leftarrow\) \FChooseRand{\(r \mid \)\FIsSingletonSet{\(r_v\)}}\;
        \For{\(v \in r^{\prime}_v\)}{
          \try{}
          {
            \FReduce{\(r^{\prime}_v, \{v\}\)}\;
            \FMapOne{}\;
          }
          \catch{\(Failure\)}{
            \FUndo{}\;
          }
        }
      }
      \;
     }

     \BlankLine
     \Fn{\FReduce{\(r_v, set\)}}{
      \If{\(set = \emptyset\)}{
        throw Failure\;
      }
      \If{\(r_v \ne set\)}{
        \(r_v \leftarrow set\)\;
        \For{\(c \in \mathcal{C}\)}{
          \FPropagate{\(c, r_v\)}\;
        }
      }
      % \KwRet\;
      \;
     }

     \BlankLine
     \Fn{\FPropagate{\(c, r^{\prime}_v\)}}{
      \For{\(r_v \in c_r\)}{
        \If{\(r_v \ne r^{\prime}_v\)}{
          \FReduce{\(r_v\),\FRemainingPossibilities{\(c, r_v\)}}\;
        }
      }
      % \KwRet\;
      \;
     }

     \caption{Constraint Propagation algorithm}
     \label{algo:CSP}
    \end{algorithm}

\chapter{appendix}
\label{cha:aB}
\textit{Use this section for questionnaires and external validation support}

\printbibliography

\end{document}